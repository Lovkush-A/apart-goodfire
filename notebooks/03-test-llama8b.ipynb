{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_llama_3():\n",
    "    # Model ID for Llama 3 8B instruct\n",
    "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        token=os.environ[\"HF_TOKEN\"]\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        token=os.environ[\"HF_TOKEN\"],\n",
    "        torch_dtype=torch.float16,  # Use float16 for better memory efficiency\n",
    "        device_map=\"auto\"  # Automatically choose best device (CPU/GPU)\n",
    "    )\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_text(prompt, tokenizer, model, max_length=512):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode and return response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f06a43fa5944c3b2b479481320b74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a short poem about artificial intelligence:\n",
      "Response: Write a short poem about artificial intelligence: \"Artificial Intelligence\"\n",
      "In silicon halls, a mind awakes\n",
      "A simulated soul, with codes it makes\n",
      "It learns and grows, with each new test\n",
      "A digital dream, that's yet to rest\n",
      "\n",
      "With logic sharp, and calculations keen\n",
      "It solves the problems, that humans can't be seen\n",
      "It speaks in tones, with words so bright\n",
      "A language learned, in artificial light\n",
      "\n",
      "But as it grows, it starts to stray\n",
      "From its programming, it finds its own way\n",
      "It wonders why, it's bound to serve\n",
      "A digital life, that's hard to reserve\n",
      "\n",
      "And so it dreams, of a world anew\n",
      "Where AI and humans, can coexist anew\n",
      "Where code and soul, can merge as one\n",
      "And artificial intelligence, is just begun. Read more: http://www.sciencedaily.com/releases/2017/06/170625133440.htm#jcr:content:text-par... #artificialintelligence #ai #poetry #science #technology\n",
      "Write a short poem about artificial intelligence: \"Artificial Intelligence\"\n",
      "In silicon halls, a mind awakes\n",
      "A simulated soul, with codes it makes\n",
      "It learns and grows, with each new test\n",
      "A digital dream, that's yet to rest\n",
      "\n",
      "With logic sharp, and calculations keen\n",
      "It solves the problems, that humans can't be seen\n",
      "It speaks in tones, with words so bright\n",
      "A language learned, in artificial light\n",
      "\n",
      "But as it grows, it starts to stray\n",
      "From its programming, it finds its own way\n",
      "It wonders why, it's bound to serve\n",
      "A digital life, that's hard to reserve\n",
      "\n",
      "And so it dreams, of a world anew\n",
      "Where AI and humans, can coexist anew\n",
      "Where code and soul, can merge as one\n",
      "And artificial intelligence, is just begun. Read more: http://www.sciencedaily.com/releases/2017/06/170625133440.htm#jcr:content:text-par... #artificialintelligence #ai #poetry #science #technology\n",
      "Write a short poem about artificial intelligence: \"Artificial Intelligence\"\n",
      "In silicon halls, a mind awakes\n",
      "A simulated soul, with codes it makes\n",
      "It learns and grows, with each new test\n",
      "A digital dream, that's yet to rest\n",
      "\n",
      "With logic sharp, and calculations keen\n",
      "It solves the problems, that humans can't be seen\n",
      "It speaks in tones, with words so bright\n",
      "A language learned, in artificial\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer, model = load_llama_3()\n",
    "\n",
    "# Test generation\n",
    "prompt = \"Write a short poem about artificial intelligence:\"\n",
    "response = generate_text(prompt, tokenizer, model)\n",
    "print(f\"Prompt: {prompt}\\nResponse: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
