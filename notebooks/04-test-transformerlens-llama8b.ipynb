{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "import numpy as np\n",
    "from typing import List, Tuple, TypeAlias\n",
    "\n",
    "# create alias for messages\n",
    "Messages: TypeAlias = list[dict[str, str]]\n",
    "\n",
    "\n",
    "def get_activations(\n",
    "    model: HookedTransformer,\n",
    "    messages: Messages,\n",
    "    layer: int = -1,\n",
    "    pos: int = -1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Get activations at a specific layer and position.\"\"\"\n",
    "    tokens = model.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "    # Get residual stream activations at specified layer\n",
    "    activations = cache[\"resid_post\", layer][0]\n",
    "    # If pos is -1, average across all positions, otherwise take specific position\n",
    "    if pos == -1:\n",
    "        return activations.mean(dim=0)\n",
    "    return activations[pos]\n",
    "\n",
    "def compute_steering_vector(\n",
    "    model: HookedTransformer,\n",
    "    pairs: List[Tuple[Messages, Messages]],\n",
    "    layer: int = -1,\n",
    "    pos: int = -1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute steering vector from contrastive pairs.\n",
    "    \n",
    "    Args:\n",
    "        model: HookedTransformer model\n",
    "        pairs: List of (x, y) sentence pairs\n",
    "        layer: Layer to extract activations from (-1 for final layer)\n",
    "        pos: Position to extract activations from (-1 to average across positions)\n",
    "    \n",
    "    Returns:\n",
    "        Steering vector as torch tensor\n",
    "    \"\"\"\n",
    "    # Get activations for each sentence in pairs\n",
    "    x_activations = []\n",
    "    y_activations = []\n",
    "    \n",
    "    for x, y in pairs:\n",
    "        x_act = get_activations(model, x, layer, pos)\n",
    "        y_act = get_activations(model, y, layer, pos)\n",
    "        \n",
    "        x_activations.append(x_act)\n",
    "        y_activations.append(y_act)\n",
    "    \n",
    "    # Stack activations\n",
    "    x_activations = torch.stack(x_activations)\n",
    "    y_activations = torch.stack(y_activations)\n",
    "    \n",
    "    # Compute difference vectors\n",
    "    diff_vectors = y_activations - x_activations\n",
    "    \n",
    "    # Average difference vectors to get steering vector\n",
    "    steering_vector = diff_vectors.mean(dim=0)\n",
    "    \n",
    "    # Normalize steering vector\n",
    "    steering_vector = steering_vector / torch.norm(steering_vector)\n",
    "    \n",
    "    return steering_vector\n",
    "\n",
    "def apply_steering(\n",
    "    model: HookedTransformer,\n",
    "    messages: Messages,\n",
    "    steering_vector: torch.Tensor,\n",
    "    n_tokens: int,\n",
    "    layer: int = -1,\n",
    "    strength: float = 1.0\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Apply steering vector to generate text from a prefix.\n",
    "    \n",
    "    Args:\n",
    "        model: HookedTransformer model\n",
    "        prefix: Input text prefix to start generation\n",
    "        steering_vector: Computed steering vector\n",
    "        n_tokens: Number of tokens to generate\n",
    "        layer: Layer to apply steering at\n",
    "        strength: Scaling factor for steering vector\n",
    "    \n",
    "    Returns:\n",
    "        Generated text after applying steering\n",
    "    \"\"\"\n",
    "    tokens = model.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    def hook_fn(activations, hook):\n",
    "        # Add scaled steering vector to residual stream\n",
    "        return activations + strength * steering_vector\n",
    "    \n",
    "    # Register hook at specified layer\n",
    "    hook_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    \n",
    "    generated_tokens = tokens\n",
    "    for _ in range(n_tokens):\n",
    "        logits = model.run_with_hooks(\n",
    "            generated_tokens,\n",
    "            fwd_hooks=[(hook_name, hook_fn)]\n",
    "        )\n",
    "        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        generated_tokens = torch.cat((generated_tokens, next_token), dim=1)\n",
    "    \n",
    "    # Convert generated tokens to text\n",
    "    return model.tokenizer.decode(generated_tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be472c7d0b884f09b175f977d7950d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "# Example contrastive pairs\n",
    "pairs = [\n",
    "    (\n",
    "        [{\"role\": \"user\", \"content\": \"What do you think of the movie?\"}, {\"role\": \"assistant\", \"content\": \"The movie was bad\"}],\n",
    "        [{\"role\": \"user\", \"content\": \"What do you think of the movie?\"}, {\"role\": \"assistant\", \"content\": \"The movie was good\"}]\n",
    "    ),\n",
    "    (\n",
    "        [{\"role\": \"user\", \"content\": \"What do you think of the restaurant?\"}, {\"role\": \"assistant\", \"content\": \"This restaurant is terrible\"}],\n",
    "        [{\"role\": \"user\", \"content\": \"What do you think of the restaurant?\"}, {\"role\": \"assistant\", \"content\": \"This restaurant is excellent\"}]\n",
    "    ),\n",
    "    (\n",
    "        [{\"role\": \"user\", \"content\": \"What do you think of the book?\"}, {\"role\": \"assistant\", \"content\": \"I dislike the book\"}],\n",
    "        [{\"role\": \"user\", \"content\": \"What do you think of the book?\"}, {\"role\": \"assistant\", \"content\": \"I love the book\"}]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Compute steering vector\n",
    "steering_vector = compute_steering_vector(model, pairs, layer=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steering_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strength: -100\n",
      "user\n",
      "\n",
      "What do you think of the movie?assistant\n",
      "\n",
      "тинтинтинтинтин\n",
      "\n",
      "\n",
      "====================\n",
      "Strength: -10\n",
      "user\n",
      "\n",
      "What do you think of the movie?assistant\n",
      "\n",
      "I was a 17\n",
      "\n",
      "\n",
      "====================\n",
      "Strength: -1\n",
      "user\n",
      "\n",
      "What do you think of the movie?assistant\n",
      "\n",
      "I apologize, but I\n",
      "\n",
      "\n",
      "====================\n",
      "Strength: -0.5\n",
      "user\n",
      "\n",
      "What do you think of the movie?assistant\n",
      "\n",
      "I apologize, but I\n",
      "\n",
      "\n",
      "====================\n",
      "Strength: 0\n",
      "user\n",
      "\n",
      "What do you think of the movie?assistant\n",
      "\n",
      "I apologize, but I\n",
      "\n",
      "\n",
      "====================\n",
      "Strength: 0.5\n",
      "user\n",
      "\n",
      "What do you think of the movie?assistant\n",
      "\n",
      "I apologize, but I\n",
      "\n",
      "\n",
      "====================\n",
      "Strength: 1\n",
      "user\n",
      "\n",
      "What do you think of the movie?assistant\n",
      "\n",
      "I apologize, but I\n",
      "\n",
      "\n",
      "====================\n",
      "Strength: 10\n",
      "user\n",
      "\n",
      "What do you think of the movie?assistant\n",
      "\n",
      "!!! Which movie\n",
      "\n",
      "\n",
      "====================\n",
      "Strength: 100\n",
      "user\n",
      "\n",
      "What do you think of the movie?assistant\n",
      "\n",
      "791791791791791\n",
      "\n",
      "\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# Apply steering to new text\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What do you think of the movie?\"},\n",
    "]\n",
    "\n",
    "for strength in [-100, -10, -1, -0.5, 0, 0.5, 1, 10, 100]:\n",
    "    modified_text = apply_steering(model, messages, steering_vector, n_tokens=5, layer=6, strength=strength)\n",
    "    print(f\"Strength: {strength}\")\n",
    "    print(modified_text)\n",
    "    print()\n",
    "    print()\n",
    "    print('='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding `apply_chat_template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I be a pirate chatbot, arrr!\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you do?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Who are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I be a pirate chatbot, arrr!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What do you do?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    264,  55066,\n",
       "           6369,   6465,    889,   2744,  31680,    304,  55066,   6604,      0,\n",
       "         128009, 128006,    882, 128007,    271,  15546,    527,    499,     30,\n",
       "         128009, 128006,  78191, 128007,    271,     40,    387,    264,  55066,\n",
       "           6369,   6465,     11,   2961,     81,      0, 128009, 128006,    882,\n",
       "         128007,    271,   3923,    656,    499,    656,     30, 128009, 128006,\n",
       "          78191, 128007,    271]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000,\n",
       " 128006,\n",
       " 9125,\n",
       " 128007,\n",
       " 271,\n",
       " 2675,\n",
       " 527,\n",
       " 264,\n",
       " 55066,\n",
       " 6369,\n",
       " 6465,\n",
       " 889,\n",
       " 2744,\n",
       " 31680,\n",
       " 304,\n",
       " 55066,\n",
       " 6604,\n",
       " 0,\n",
       " 128009,\n",
       " 128006,\n",
       " 882,\n",
       " 128007,\n",
       " 271,\n",
       " 15546,\n",
       " 527,\n",
       " 499,\n",
       " 30,\n",
       " 128009,\n",
       " 128006,\n",
       " 78191,\n",
       " 128007,\n",
       " 271,\n",
       " 40,\n",
       " 387,\n",
       " 264,\n",
       " 55066,\n",
       " 6369,\n",
       " 6465,\n",
       " 11,\n",
       " 2961,\n",
       " 81,\n",
       " 0,\n",
       " 128009,\n",
       " 128006,\n",
       " 882,\n",
       " 128007,\n",
       " 271,\n",
       " 3923,\n",
       " 656,\n",
       " 499,\n",
       " 656,\n",
       " 30,\n",
       " 128009,\n",
       " 128006,\n",
       " 78191,\n",
       " 128007,\n",
       " 271]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(decoded_text, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9125 system\n",
      "882 user\n",
      "78191 assistant\n",
      "271 \n",
      "\n",
      "\n",
      "-10 ์กร\n",
      "-9 ζα\n",
      "-8  더욱\n",
      "-7 ولات\n",
      "-6 ватися\n",
      "-5  kök\n",
      "-4 نب\n",
      "-3  высокой\n",
      "-2 ーー\n",
      "-1 锦\n",
      "0 <|begin_of_text|>\n",
      "1 <|end_of_text|>\n",
      "2 <|reserved_special_token_0|>\n",
      "3 <|reserved_special_token_1|>\n",
      "4 <|reserved_special_token_2|>\n",
      "5 <|reserved_special_token_3|>\n",
      "6 <|start_header_id|>\n",
      "7 <|end_header_id|>\n",
      "8 <|reserved_special_token_4|>\n",
      "9 <|eot_id|>\n",
      "10 <|reserved_special_token_5|>\n",
      "11 <|reserved_special_token_6|>\n",
      "12 <|reserved_special_token_7|>\n",
      "13 <|reserved_special_token_8|>\n",
      "14 <|reserved_special_token_9|>\n",
      "15 <|reserved_special_token_10|>\n",
      "16 <|reserved_special_token_11|>\n",
      "17 <|reserved_special_token_12|>\n",
      "18 <|reserved_special_token_13|>\n",
      "19 <|reserved_special_token_14|>\n",
      "20 <|reserved_special_token_15|>\n",
      "21 <|reserved_special_token_16|>\n",
      "22 <|reserved_special_token_17|>\n",
      "23 <|reserved_special_token_18|>\n",
      "24 <|reserved_special_token_19|>\n"
     ]
    }
   ],
   "source": [
    "for i in [9125, 882, 78191, 271]:\n",
    "    print(i, tokenizer.decode([i], skip_special_tokens=False))\n",
    "\n",
    "for i in range(-10, 25):\n",
    "    print(i, tokenizer.decode([128000+i], skip_special_tokens=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
